{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install lightgbm\n",
    "#%pip install xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import f_oneway\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb  \n",
    "import xgboost as xgb   \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Obs</th>\n",
       "      <th>CRSP_date</th>\n",
       "      <th>CUSIP6</th>\n",
       "      <th>NCUSIP</th>\n",
       "      <th>CRSP_TICKER</th>\n",
       "      <th>PERMNO</th>\n",
       "      <th>PERMCO</th>\n",
       "      <th>PRC</th>\n",
       "      <th>SHROUT</th>\n",
       "      <th>CUSIP</th>\n",
       "      <th>...</th>\n",
       "      <th>Secondary__Shs_Ofrd___sum_of_all</th>\n",
       "      <th>Sec_Shs__as___of_Shs__Ofrd___sum</th>\n",
       "      <th>Sec_Shs__as___of_Shs_Ofrd___in__</th>\n",
       "      <th>Secondary__Shs_Ofrd____in_this_0</th>\n",
       "      <th>Serial</th>\n",
       "      <th>Shareholder_Take_Up_All_Markets_</th>\n",
       "      <th>Shares_Ofrd_as___of_Shs_Out_Bef_</th>\n",
       "      <th>Total_Shares_Offered__mil_</th>\n",
       "      <th>Simul__taneous_Offer_Cusip</th>\n",
       "      <th>Stock_Price_at_Close_of_Offer__0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>14846C</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>18886P</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>29353W</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>42212J</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>46688X</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10438</th>\n",
       "      <td>10439</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>84663K</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10439</th>\n",
       "      <td>10440</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>88588P</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10440</th>\n",
       "      <td>10441</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>941845</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10441</th>\n",
       "      <td>10442</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>973146</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10442</th>\n",
       "      <td>10443</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>98385L</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10443 rows Ã— 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Obs CRSP_date CUSIP6 NCUSIP CRSP_TICKER PERMNO PERMCO PRC SHROUT  \\\n",
       "0          1         .    NaN    NaN         NaN      .      .   .      .   \n",
       "1          2         .    NaN    NaN         NaN      .      .   .      .   \n",
       "2          3         .    NaN    NaN         NaN      .      .   .      .   \n",
       "3          4         .    NaN    NaN         NaN      .      .   .      .   \n",
       "4          5         .    NaN    NaN         NaN      .      .   .      .   \n",
       "...      ...       ...    ...    ...         ...    ...    ...  ..    ...   \n",
       "10438  10439         .    NaN    NaN         NaN      .      .   .      .   \n",
       "10439  10440         .    NaN    NaN         NaN      .      .   .      .   \n",
       "10440  10441         .    NaN    NaN         NaN      .      .   .      .   \n",
       "10441  10442         .    NaN    NaN         NaN      .      .   .      .   \n",
       "10442  10443         .    NaN    NaN         NaN      .      .   .      .   \n",
       "\n",
       "        CUSIP  ... Secondary__Shs_Ofrd___sum_of_all  \\\n",
       "0      14846C  ...                              NaN   \n",
       "1      18886P  ...                              NaN   \n",
       "2      29353W  ...                              NaN   \n",
       "3      42212J  ...                              NaN   \n",
       "4      46688X  ...                              NaN   \n",
       "...       ...  ...                              ...   \n",
       "10438  84663K  ...                              NaN   \n",
       "10439  88588P  ...                              NaN   \n",
       "10440  941845  ...                              NaN   \n",
       "10441  973146  ...                              NaN   \n",
       "10442  98385L  ...                              NaN   \n",
       "\n",
       "      Sec_Shs__as___of_Shs__Ofrd___sum Sec_Shs__as___of_Shs_Ofrd___in__  \\\n",
       "0                                  NaN                              NaN   \n",
       "1                                  NaN                              NaN   \n",
       "2                                  NaN                              NaN   \n",
       "3                                  NaN                              NaN   \n",
       "4                                  NaN                              NaN   \n",
       "...                                ...                              ...   \n",
       "10438                              NaN                              NaN   \n",
       "10439                              NaN                              NaN   \n",
       "10440                              NaN                              NaN   \n",
       "10441                              NaN                              NaN   \n",
       "10442                              NaN                              NaN   \n",
       "\n",
       "      Secondary__Shs_Ofrd____in_this_0 Serial  \\\n",
       "0                                  NaN    NaN   \n",
       "1                                  NaN    NaN   \n",
       "2                                  NaN    NaN   \n",
       "3                                  NaN    NaN   \n",
       "4                                  NaN    NaN   \n",
       "...                                ...    ...   \n",
       "10438                              NaN    NaN   \n",
       "10439                              NaN    NaN   \n",
       "10440                              NaN    NaN   \n",
       "10441                              NaN    NaN   \n",
       "10442                              NaN    NaN   \n",
       "\n",
       "      Shareholder_Take_Up_All_Markets_ Shares_Ofrd_as___of_Shs_Out_Bef_  \\\n",
       "0                                  NaN                              NaN   \n",
       "1                                  NaN                              NaN   \n",
       "2                                  NaN                              NaN   \n",
       "3                                  NaN                              NaN   \n",
       "4                                  NaN                              NaN   \n",
       "...                                ...                              ...   \n",
       "10438                              NaN                              NaN   \n",
       "10439                              NaN                              NaN   \n",
       "10440                              NaN                              NaN   \n",
       "10441                              NaN                              NaN   \n",
       "10442                              NaN                              NaN   \n",
       "\n",
       "      Total_Shares_Offered__mil_ Simul__taneous_Offer_Cusip  \\\n",
       "0                            NaN                        NaN   \n",
       "1                            NaN                        NaN   \n",
       "2                            NaN                        NaN   \n",
       "3                            NaN                        NaN   \n",
       "4                            NaN                        NaN   \n",
       "...                          ...                        ...   \n",
       "10438                      5.751                        NaN   \n",
       "10439                      1.149                        NaN   \n",
       "10440                     12.000                        NaN   \n",
       "10441                      4.833                        NaN   \n",
       "10442                      1.000                        NaN   \n",
       "\n",
       "      Stock_Price_at_Close_of_Offer__0  \n",
       "0                                  NaN  \n",
       "1                                  NaN  \n",
       "2                                  NaN  \n",
       "3                                  NaN  \n",
       "4                                  NaN  \n",
       "...                                ...  \n",
       "10438                              NaN  \n",
       "10439                              NaN  \n",
       "10440                              NaN  \n",
       "10441                              NaN  \n",
       "10442                              NaN  \n",
       "\n",
       "[10443 rows x 112 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original Datafile is called Raw Data.\n",
    "raw_data = pd.read_csv('SEO CRSP 2000 - 2009.csv')\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Marketplaces Table:\n",
      "     Marketplace  Count\n",
      "0   U.S. Private   5991\n",
      "1    U.S. Public   4111\n",
      "2   Registration    172\n",
      "3      Withdrawn    168\n",
      "4  India Private      1\n"
     ]
    }
   ],
   "source": [
    "# Assuming the dataset is loaded as shown in your original code\n",
    "# Get unique marketplaces\n",
    "unique_marketplaces = raw_data['Marketplace'].unique()\n",
    "\n",
    "# Create a DataFrame for better table display\n",
    "marketplace_df = pd.DataFrame({\n",
    "    'Marketplace': unique_marketplaces\n",
    "})\n",
    "\n",
    "# Add a count of occurrences for each marketplace\n",
    "marketplace_counts = raw_data['Marketplace'].value_counts()\n",
    "marketplace_df['Count'] = marketplace_df['Marketplace'].map(marketplace_counts)\n",
    "\n",
    "# Sort by count in descending order\n",
    "marketplace_df = marketplace_df.sort_values('Count', ascending=False)\n",
    "\n",
    "# Reset index to make it look cleaner\n",
    "marketplace_df = marketplace_df.reset_index(drop=True)\n",
    "\n",
    "# Display the table\n",
    "print(\"Unique Marketplaces Table:\")\n",
    "print(marketplace_df)\n",
    "\n",
    "# Optional: If you want a more formatted display, you can use:\n",
    "# from IPython.display import display\n",
    "# display(marketplace_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers by Issue Date with Count > 1:\n",
      "     Issue_Date Ticker_Symbol  Count\n",
      "0    18/11/2009         LBTYA      6\n",
      "1    04/03/2009             D      5\n",
      "2    24/07/2003           STZ      4\n",
      "3    28/02/2002            GM      4\n",
      "4    25/04/2007          IRLD      4\n",
      "..          ...           ...    ...\n",
      "394  11/08/2009          GNMT      2\n",
      "395  11/07/2007           NEM      2\n",
      "396  11/06/2008           BRS      2\n",
      "397  11/06/2007           TAP      2\n",
      "398  31/12/2008          VNGM      2\n",
      "\n",
      "[399 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Assuming clean_data is your DataFrame\n",
    "# Group by Issue_Date and Ticker_Symbol, then count occurrences\n",
    "ticker_by_issue = raw_data.groupby(['Issue_Date', 'Ticker_Symbol']).size().reset_index(name='Count')\n",
    "\n",
    "# Filter for counts greater than 1\n",
    "duplicate_tickers = ticker_by_issue[ticker_by_issue['Count'] > 1]\n",
    "\n",
    "# Sort by Count in descending order for better readability\n",
    "duplicate_tickers = duplicate_tickers.sort_values('Count', ascending=False)\n",
    "\n",
    "# Reset index for clean display\n",
    "duplicate_tickers = duplicate_tickers.reset_index(drop=True)\n",
    "\n",
    "# Display the table\n",
    "print(\"Tickers by Issue Date with Count > 1:\")\n",
    "print(duplicate_tickers)\n",
    "\n",
    "# Optional: If you want to format the output differently\n",
    "# print(duplicate_tickers.to_string(index=False))  # Without index\n",
    "# or\n",
    "# from IPython.display import display\n",
    "# display(duplicate_tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:\n",
      " CUSIP6                               3345\n",
      "NCUSIP                               3345\n",
      "CRSP_TICKER                          3471\n",
      "__Digit_CUSIP                        4619\n",
      "Ticker_Symbol                         957\n",
      "                                    ...  \n",
      "Shareholder_Take_Up_All_Markets_    10324\n",
      "Shares_Ofrd_as___of_Shs_Out_Bef_     8239\n",
      "Total_Shares_Offered__mil_           2878\n",
      "Simul__taneous_Offer_Cusip           9584\n",
      "Stock_Price_at_Close_of_Offer__0     6828\n",
      "Length: 83, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for all columns with missing values\n",
    "missing_data = raw_data.isnull().sum()\n",
    "print(\"Columns with missing values:\\n\", missing_data[missing_data > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unwanted columns from the dataset.\n",
    "clean_data = raw_data.loc[raw_data['Marketplace'] == \"U.S. Public\"] # Removed all observations other than \"US Public\" companies.\n",
    "clean_data = clean_data.dropna(subset=[\"Ticker_Symbol\"]) # Removed NAs from the Ticker column.\n",
    "clean_data = clean_data.dropna(subset=[\"IPO_Flag__Y_N_\"]) # Removed NAs from the IPO flag column, since we are only interested in secondary sale of shares.\n",
    "clean_data = clean_data.drop_duplicates(subset=[\"Ticker_Symbol\", \"Issue_Date\"], keep=\"first\") # Removed duplicates by combining ticker and issue date.\n",
    "clean_data = clean_data.dropna(axis=1, how='all') # Drop column if ALL values are NAs\n",
    "clean_data = clean_data.reset_index(drop=True).drop(columns=[\"Issue_Date1\", \"Issue_Date2\" , \"Date_Last_Updated\" , \"Date_Created\", \"Year\" ]) # Remove multiple date columns\n",
    "clean_data = clean_data.reset_index(drop=True).drop(columns=[\"CRSP_date\", \"CUSIP6\" , \"NCUSIP\" ,\"CRSP_TICKER\", \"PERMCO\", \"PRC\" , \"SHROUT\", \"CUSIP\" ,\"Main_SIC_Code\",\"Master_Deal_Type\",\"Simul__taneous_Offer_Cusip\", \"__Digit_CUSIP\" , \"Investor_Cusip\" , \"Issuer_Borrower_SEDOL\", \"Deal_Number\"]) # Remove multiple identity columns\n",
    "clean_data = clean_data.reset_index(drop=True).drop(columns=[\"Payment_Date\", \"Cur__rency\" , \"Marketplace\" ,\"Stock_Price_at_Close_of_Offer__1\", \"Domicile_Nation_Code\"]) \n",
    "clean_data['Bookrunner_s_'] = clean_data['Bookrunner_s_'].replace('NOT-AVAILABLE', \"Unknown\")\n",
    "clean_data['Bookrunner_s_'] = clean_data['Bookrunner_s_'].replace('NOTAPP', \"Unknown\")\n",
    "clean_data['Gross_Spread_per_share_or_bond__'] = clean_data['Gross_Spread_per_share_or_bond__'].replace('na', 0)\n",
    "clean_data['Gross_Spread_as___of__Prncpl_Amt'] = clean_data['Gross_Spread_as___of__Prncpl_Amt'].replace('na', 0)\n",
    "clean_data['Filing_Date'] = clean_data['Filing_Date'].replace('.', pd.NA)\n",
    "clean_data['PERMNO'] = clean_data['PERMNO'].replace('.', pd.NA)\n",
    "clean_data = clean_data.drop(columns=\"Over_Subscription_Flag\")\n",
    "clean_data = clean_data.drop(columns=\"Prices_Update\")\n",
    "\n",
    "# Bulk Drop of all unwanted columns.\n",
    "columns_to_drop = [\n",
    "    'Obs', 'Issue_Date3', 'Manage__ment_Fee____',\n",
    "    'Under__writing_Fee____', 'Selling_Conces__sion____', 'Re__allow__ance_Fee____',\n",
    "    'Management_Fee_as____of_Prncpl_A', 'Underwriting_Fee_as___of__Prncpl',\n",
    "    \"Settlement_Date\",'Selling__Concession__as___of__Pr', 'Reallowance_Fee_as___of_Prncpl_A',\n",
    "    'Gross__Spread____in_this__Mkt___', 'Principal__Amt___sum__of_all_Mkt',\n",
    "    'Proceeds__Amt___sum__of_all_Mkts', 'Amt_Filed____in_this_Mkt_____mil',\n",
    "    'Total_Dollar_Amount_Filed', 'Amended_Secondary_Amount_Filed_T',\n",
    "    'Amended__Secondary__Shs_Filed___', 'Amended__Sec_Shs__Filed_as____of',\n",
    "    'Sec_Shrs_Filed_as___of_Shares_Am', 'Amended_Secondary_Shares_Filed',\n",
    "    'Amended__Shs_Filed____in_this_Mk', 'Shares__Offered____in_this_Mkt1',\n",
    "    'Original_Middle_of_Filing_Price0', 'Gross__Spread____in_this__Mkt__0',\n",
    "    'Total_Manage__ment_Fee____', 'Mgmt_as___Gross_Spread', 'Total____Re__allow__ance_Fee____',\n",
    "    'Total_Re__allow__ance_Fee____mil', 'Total_Under__writing_Fee____mil_',\n",
    "    'Proceeds_Amt___in__this_Mkt____0', 'Secondary_Amount_Filed_This_Mkt_',\n",
    "    'Secondary_Amount_Offered_This_Mk', 'Sec_Shs_as____of_Shs__Out_Aft__O',\n",
    "    'Sec_Shs_as____of_Shs__Out_Bef__O', 'Secondary__Shs_Filed____sum_of__',\n",
    "    'Sec_Shs__Filed_as____of_Shs__Fil', 'Original_Secondary_Shares_Filed',\n",
    "    'Secondary__Shs_Ofrd___sum_of__al', 'Secondary__Shs_Ofrd___sum_of_all',\n",
    "    'Sec_Shs__as___of_Shs__Ofrd___sum', 'Sec_Shs__as___of_Shs_Ofrd___in__',\n",
    "    'Secondary__Shs_Ofrd____in_this_0', 'Total_Shares_Offered__mil_' , \"Date_Filing_Amended\"\n",
    "]\n",
    "# Drop the columns (ignore errors if some columns are missing)\n",
    "clean_data = clean_data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3650 entries, 0 to 3649\n",
      "Data columns (total 33 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   PERMNO                            3548 non-null   object \n",
      " 1   Ticker_Symbol                     3650 non-null   object \n",
      " 2   Issue_Date                        3650 non-null   object \n",
      " 3   Primary_Exchange_Where_Issuer_s_  3650 non-null   object \n",
      " 4   Bookrunner_s_                     3650 non-null   object \n",
      " 5   Gross_Spread_per_share_or_bond__  3650 non-null   object \n",
      " 6   Gross_Spread_as___of__Prncpl_Amt  3650 non-null   object \n",
      " 7   Principal_Amount____mil_          3650 non-null   object \n",
      " 8   Proceeds_Amt___in__this_Mkt____m  3650 non-null   object \n",
      " 9   Offer_Price                       3650 non-null   object \n",
      " 10  Type_of_Security                  3650 non-null   object \n",
      " 11  Primary_Exchange_Where_Issue_Wil  3650 non-null   object \n",
      " 12  Filing_Date                       3598 non-null   object \n",
      " 13  Original_Middle_of_Filing_Price_  3514 non-null   float64\n",
      " 14  Shares__Filed____in_this_Mkt      3216 non-null   float64\n",
      " 15  IPO_Flag__Y_N_                    3650 non-null   object \n",
      " 16  Shares__Offered____in_this_Mkt    3650 non-null   float64\n",
      " 17  Secondary__Shs_Ofrd____in_this_M  970 non-null    float64\n",
      " 18  Yesterday_s_Stock_Price           3642 non-null   float64\n",
      " 19  Deal_Size__as_Pct_of_Market_Cap   3356 non-null   float64\n",
      " 20  Equity___Equity_Related_Simultan  3650 non-null   object \n",
      " 21  Common_Equity____mil_             2971 non-null   float64\n",
      " 22  Common_Equity_Before_the_Offerin  3472 non-null   float64\n",
      " 23  State                             3650 non-null   object \n",
      " 24  Overallot__Amt_Option___in_this_  3650 non-null   float64\n",
      " 25  Overallot__Amt_Sold___sum_of_all  1714 non-null   float64\n",
      " 26  Price_Current                     3640 non-null   float64\n",
      " 27  __Of_Insider_Shares_After_Offer   528 non-null    float64\n",
      " 28  __Of_Insider_Shares_Before_Offer  620 non-null    float64\n",
      " 29  Sec_Shrs_Filed_as___of_Shares     948 non-null    float64\n",
      " 30  Shareholder_Take_Up_All_Markets_  111 non-null    float64\n",
      " 31  Shares_Ofrd_as___of_Shs_Out_Bef_  2151 non-null   float64\n",
      " 32  Stock_Price_at_Close_of_Offer__0  3535 non-null   float64\n",
      "dtypes: float64(17), object(16)\n",
      "memory usage: 941.1+ KB\n"
     ]
    }
   ],
   "source": [
    "clean_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"/\" and \"-\" from Bookrunner_s_ column\n",
    "clean_data['Bookrunner_s_'] = clean_data['Bookrunner_s_'].str.replace('/', '')\n",
    "clean_data['Bookrunner_s_'] = clean_data['Bookrunner_s_'].str.replace('-', '')\n",
    "clean_data['Bookrunner_s_'] = clean_data['Bookrunner_s_'].str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all unique Tickers and PERMNOs IDs into a file.\n",
    "all_tickers = clean_data[[\"Ticker_Symbol\",\"PERMNO\"]].drop_duplicates()\n",
    "all_tickers.to_csv(\"all_tickers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling all tickers with missing Stock closing Price.\n",
    "missing_stock_price = clean_data[clean_data['Stock_Price_at_Close_of_Offer__0'].isna()][[\"Ticker_Symbol\"]].drop_duplicates()\n",
    "\n",
    "# Save the result to a CSV file\n",
    "missing_stock_price.to_csv(\"missing_stock_price_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique key to aid in merge with other datasets.\n",
    "clean_data['Name_Key_tic'] = (clean_data['Ticker_Symbol']) + \"_\" + (clean_data['Issue_Date'].astype(str))\n",
    "clean_data['Name_Key_PERMNO'] = (clean_data['PERMNO']).astype(str) + \"_\" + (clean_data['Issue_Date'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Stock close price from WRDS dataset to merge with original dataset - Using Ticker & Issue Date as Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stock price data.\n",
    "close_price_1 = pd.read_csv('close_price_data.csv')\n",
    "close_price_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted columns from the dataset.\n",
    "close_price_1 = close_price_1.drop(close_price_1.columns[: 9], axis = 1)\n",
    "close_price_1 = close_price_1.drop(columns=\"ccmbegdt\")\n",
    "close_price_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for easy understanding\n",
    "close_price_1.rename(columns={'prccd': 'stock_close_price_wrds', 'datadate': 'date' , 'tic': 'ticker'}, inplace=True)\n",
    "\n",
    "# Convert from YYYY-MM-DD to DD/MM/YYYY so that it is consistent with the 'Issue_Date' column in the main dataset.\n",
    "close_price_1['date'] = pd.to_datetime(close_price_1['date']).dt.strftime('%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a unique key for the stock price data to merge with the main dataset.\n",
    "close_price_1['Name_Key_tic'] = (close_price_1['ticker']) + \"_\" + (close_price_1['date'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_price_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging closing price with the original dataset to fill in NAs\n",
    "clean_data = pd.merge(clean_data, close_price_1, on='Name_Key_tic' , how = \"left\")\n",
    "\n",
    "# Fill in the NAs by using the merged data into the original column.\n",
    "clean_data['Stock_Price_at_Close_of_Offer__0'] = clean_data['Stock_Price_at_Close_of_Offer__0'].fillna(clean_data['stock_close_price_wrds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data.drop(columns=[\"stock_close_price_wrds\", \"ticker\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data to excel to check if the merge was successful\n",
    "\n",
    "#clean_data.to_csv(\"clean_data_post_merge1.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Stock close price from WRDS dataset to merge with original dataset - Using PERMNO & Issue Date as Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling all PERMNO IDs with corresponding missing Stock close price into a file.\n",
    "missing_stock_price = clean_data[clean_data['Stock_Price_at_Close_of_Offer__0'].isna()][[\"PERMNO\"]].drop_duplicates()\n",
    "\n",
    "# Save the result to a CSV file\n",
    "missing_stock_price.to_csv(\"missing_stock_price_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "close_price_2 = pd.read_csv('close_price_data_2.csv')\n",
    "close_price_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unwanted columns from the dataset.\n",
    "close_price_2 = close_price_2.drop(close_price_2.columns[: 4], axis = 1)\n",
    "close_price_2 = close_price_2.drop(columns= [\"ccmbegdt\", \"LPERMCO\" , \"LINKDT\" , \"LINKENDDT\" , \"iid\" , \"tic\"])\n",
    "close_price_2['datadate'] = pd.to_datetime(close_price_2['datadate']).dt.strftime('%d/%m/%Y')\n",
    "close_price_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a unique key to merge with the original dataset. \n",
    "close_price_2['Name_Key_PERMNO'] = (close_price_2['LPERMNO']).astype(str) + \"_\" + (close_price_2['datadate'].astype(str))\n",
    "close_price_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging with the original dataset\n",
    "clean_data = pd.merge(clean_data, close_price_2, on='Name_Key_PERMNO' , how = \"left\")\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling NAs into the original column. \n",
    "clean_data['Stock_Price_at_Close_of_Offer__0'] = clean_data['Stock_Price_at_Close_of_Offer__0'].fillna(clean_data['prccd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data to excel to check if the merge was successful\n",
    "\n",
    "#Clean_data.to_csv(\"clean_data_post_merge2.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data.dropna(subset=[\"Stock_Price_at_Close_of_Offer__0\"])\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop all columns with missing data more than 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of columns it will drop\n",
    "missing_threshold = 0.1\n",
    "\n",
    "missing_percent = clean_data.isnull().mean()\n",
    "\n",
    "columns_to_drop = missing_percent[missing_percent > missing_threshold].index.tolist()\n",
    "\n",
    "nullvalues = len(set(columns_to_drop))\n",
    "print(nullvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping columns\n",
    "clean_data = clean_data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for columns with missing values\n",
    "missing_data = clean_data.isnull().sum()\n",
    "print(\"Columns with missing values:\\n\", missing_data[missing_data > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data.sort_values(by=['Ticker_Symbol', 'Issue_Date'])\n",
    "\n",
    "# Fill missing Yesterday_s_Stock_Price with the last populated value for each Ticker_Symbol\n",
    "clean_data['Yesterday_s_Stock_Price'] = clean_data.groupby('Ticker_Symbol')['Yesterday_s_Stock_Price'].ffill()\n",
    "\n",
    "\n",
    "clean_data = clean_data.sort_values(by=['Ticker_Symbol', 'Filing_Date'])\n",
    "# Fill missing Yesterday_s_Stock_Price with the last populated value for each Ticker_Symbol\n",
    "clean_data['Filing_Date'] = clean_data.groupby('Ticker_Symbol')['Filing_Date'].ffill()\n",
    "\n",
    "clean_data = clean_data.sort_values(by=['Ticker_Symbol', 'PERMNO'])\n",
    "\n",
    "# Fill missing Yesterday_s_Stock_Price with the last populated value for each Ticker_Symbol\n",
    "clean_data['PERMNO'] = clean_data.groupby('Ticker_Symbol')['PERMNO'].ffill()\n",
    "\n",
    "missing_data = clean_data.isnull().sum()\n",
    "print(\"Columns with missing values:\\n\", missing_data[missing_data > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert columns to numeric\n",
    "clean_data['Yesterday_s_Stock_Price'] = pd.to_numeric(clean_data['Yesterday_s_Stock_Price'], errors='coerce')\n",
    "clean_data['Principal_Amount____mil_'] = pd.to_numeric(clean_data['Principal_Amount____mil_'], errors='coerce')\n",
    "clean_data['Common_Equity_Before_the_Offerin'] = pd.to_numeric(clean_data['Common_Equity_Before_the_Offerin'], errors='coerce')\n",
    "clean_data['Deal_Size__as_Pct_of_Market_Cap'] = pd.to_numeric(clean_data['Deal_Size__as_Pct_of_Market_Cap'], errors='coerce')\n",
    "\n",
    "# Estimate Shares Outstanding (absolute value for negatives)\n",
    "clean_data['Shares_Outstanding'] = (clean_data['Common_Equity_Before_the_Offerin'] / clean_data['Yesterday_s_Stock_Price']).abs()\n",
    "\n",
    "# Calculate Market Cap (in millions USD)\n",
    "clean_data['Market_Cap'] = clean_data['Shares_Outstanding'] * clean_data['Yesterday_s_Stock_Price']\n",
    "\n",
    "# Calculate Deal_Size__as_Pct_of_Market_Cap\n",
    "clean_data['Calculated_Deal_Size_as_Pct_of_Market_Cap'] = (clean_data['Principal_Amount____mil_'] / clean_data['Market_Cap']) * 100\n",
    "\n",
    "# Fill missing values in Deal_Size__as_Pct_of_Market_Cap with the calculated values\n",
    "clean_data['Deal_Size__as_Pct_of_Market_Cap'] = clean_data['Deal_Size__as_Pct_of_Market_Cap'].fillna(clean_data['Calculated_Deal_Size_as_Pct_of_Market_Cap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unwanted columns\n",
    "clean_data = clean_data.drop(columns=[\"Original_Middle_of_Filing_Price_\"])\n",
    "clean_data = clean_data.dropna(subset=[\"Shares_Outstanding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for remaining columns with misssing values\n",
    "missing_data = clean_data.isnull().sum()\n",
    "print(\"Columns with missing values:\\n\", missing_data[missing_data > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with missing values as it does not make siginificant portion of the dataset\n",
    "clean_data = clean_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for remaining columns with misssing values\n",
    "missing_data = clean_data.isnull().sum()\n",
    "print(\"Columns with missing values:\\n\", missing_data[missing_data > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing value indicators\n",
    "missing_indicators = ['na', '.', '']\n",
    "clean_data.replace(missing_indicators, pd.NA, inplace=True)\n",
    "\n",
    "# Force converting columns to numeric\n",
    "numeric_cols = ['Gross_Spread_per_share_or_bond__', 'Gross_Spread_as___of__Prncpl_Amt', \n",
    "                'Principal_Amount____mil_', 'Proceeds_Amt___in__this_Mkt____m', 'Offer_Price', \n",
    "                'Shares__Offered____in_this_Mkt', 'Yesterday_s_Stock_Price', \n",
    "                'Common_Equity_Before_the_Offerin', 'Overallot__Amt_Option___in_this_', \n",
    "                'Price_Current', 'Stock_Price_at_Close_of_Offer__0', 'Shares_Outstanding', \n",
    "                'Market_Cap', 'Calculated_Deal_Size_as_Pct_of_Market_Cap']\n",
    "for col in numeric_cols:\n",
    "    clean_data[col] = pd.to_numeric(clean_data[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Additional Datasets - 1-Month US Interest Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_rates = pd.read_csv('interest_rates.csv')\n",
    "interest_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_rates['observation_date'] = pd.to_datetime(interest_rates['observation_date']).dt.strftime('%d/%m/%Y')\n",
    "interest_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = pd.merge(clean_data, interest_rates, left_on='Issue_Date' ,right_on= \"observation_date\", how = \"left\")\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.rename(columns={'DGS1MO': '1_month_interest_rate'}, inplace=True)\n",
    "clean_data = clean_data.drop(columns=\"observation_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill DGS1MO using forward fill after sorting my ticker and issue date.\n",
    "clean_data = clean_data.sort_values(by=['Issue_Date'])  # Sort by date\n",
    "clean_data['1_month_interest_rate'] = clean_data['1_month_interest_rate'].ffill()  # Forward-fill only DGS1MO\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for any missing values\n",
    "missing_data = clean_data.isnull().sum()\n",
    "print(\"Columns with missing values:\\n\", missing_data[missing_data > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataset to csv\n",
    "clean_data.to_csv('clean_data_post_merge3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Additional Datasets - S&P 500 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load excel format S&P 500 data\n",
    "sp500 = pd.read_excel('S&P500_capital_IQ.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting time into a format similar to the original dataset so that merging is easy\n",
    "sp500['Pricing Date'] = pd.to_datetime(sp500['Pricing Date']).dt.strftime('%d/%m/%Y')\n",
    "sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the data using the date column\n",
    "clean_data = pd.merge(clean_data, sp500, left_on='Issue_Date' ,right_on= \"Pricing Date\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for any missing values\n",
    "missing_data = clean_data.isnull().sum()\n",
    "print(\"Columns with missing values:\\n\", missing_data[missing_data > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by date and forward fill for missing values\n",
    "clean_data = clean_data.sort_values(by=['Issue_Date'])\n",
    "clean_data['S&P 500 Price Return-Index Value (Daily)'] = clean_data['S&P 500 Price Return-Index Value (Daily)'].ffill()\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Additional Datasets - Unemployment Rate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading unemployement data\n",
    "unemployment = pd.read_csv('unemployment_rate_FRED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting time into a format similar to the original dataset so that merging is easy\n",
    "unemployment['observation_date'] = pd.to_datetime(unemployment['observation_date']).dt.strftime('%d/%m/%Y')\n",
    "unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column \"Year\" from the observation_date column\n",
    "unemployment['Year'] = pd.to_datetime(unemployment['observation_date']).dt.year\n",
    "unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting issue date by its delimiter into 3 columns\n",
    "clean_data[['Day', 'Month', 'Year']] = clean_data['Issue_Date'].str.split('/', expand=True)\n",
    "clean_data\n",
    "\n",
    "# Droping day and month columns\n",
    "clean_data = clean_data.drop(columns=['Day', 'Month'])\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Year column in both datasets as object\n",
    "unemployment['Year'] = unemployment['Year'].astype(str)\n",
    "clean_data['Year'] = clean_data['Year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the unemployment data with the full dataset\n",
    "clean_data = pd.merge(clean_data, unemployment, left_on='Year' ,right_on= \"Year\", how = \"left\")\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.rename(columns={'UNRATE': '1_month_interest_rate'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "missing_data = clean_data.isnull().sum()\n",
    "print(\"Columns with missing values:\\n\", missing_data[missing_data > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Additional Datasets - GDP per Capita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading GDP dataset\n",
    "gdp = pd.read_csv('gdp_per_capita.csv')\n",
    "gdp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert obeeravtion_date to only year format\n",
    "gdp['observation_date'] = pd.to_datetime(gdp['observation_date']).dt.strftime('%Y')\n",
    "gdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the gdp data with the full dataset\n",
    "clean_data = pd.merge(clean_data, gdp, left_on='Year' ,right_on= \"observation_date\", how = \"left\")\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.rename(columns={'A939RC0Q052SBEA': 'GDP_per_capita'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "missing_data = clean_data.isnull().sum()\n",
    "print(\"Columns with missing values:\\n\", missing_data[missing_data > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of columns to drop\n",
    "columns_to_drop = ['Name_Key_tic', 'Name_Key_PERMNO', \"Common_Equity_Before_the_Offerin\", \n",
    "                   'observation_date_x', 'Pricing Date', \"observation_date_y\",\n",
    "                   \"PERMNO\" , \"Ticker_Symbol\", \"Filing_Date\", \"Calculated_Deal_Size_as_Pct_of_Market_Cap\"]\n",
    "\n",
    "clean_data = clean_data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns for better understanding\n",
    "#clean_data.rename(columns={'DGS1MO': '1_month_interest_rate'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Underpricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['Underpricing'] = (clean_data['Stock_Price_at_Close_of_Offer__0'] - clean_data['Offer_Price']) / clean_data['Offer_Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Underpricing Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create uderpricing flag column. If underpricing is greater than 0.15%, then the flag is 1, else 0\n",
    "clean_data['Underpricing_Flag'] = (clean_data['Underpricing'] > 0.03).astype(int)\n",
    "clean_data = clean_data.drop(columns=\"Underpricing\")\n",
    "\n",
    "# reset index\n",
    "clean_data = clean_data.reset_index(drop=True)\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.to_csv('final_clean_dataset_assignment_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3650 entries, 0 to 3649\n",
      "Data columns (total 33 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   PERMNO                            3548 non-null   object \n",
      " 1   Ticker_Symbol                     3650 non-null   object \n",
      " 2   Issue_Date                        3650 non-null   object \n",
      " 3   Primary_Exchange_Where_Issuer_s_  3650 non-null   object \n",
      " 4   Bookrunner_s_                     3650 non-null   object \n",
      " 5   Gross_Spread_per_share_or_bond__  3650 non-null   object \n",
      " 6   Gross_Spread_as___of__Prncpl_Amt  3650 non-null   object \n",
      " 7   Principal_Amount____mil_          3650 non-null   object \n",
      " 8   Proceeds_Amt___in__this_Mkt____m  3650 non-null   object \n",
      " 9   Offer_Price                       3650 non-null   object \n",
      " 10  Type_of_Security                  3650 non-null   object \n",
      " 11  Primary_Exchange_Where_Issue_Wil  3650 non-null   object \n",
      " 12  Filing_Date                       3598 non-null   object \n",
      " 13  Original_Middle_of_Filing_Price_  3514 non-null   float64\n",
      " 14  Shares__Filed____in_this_Mkt      3216 non-null   float64\n",
      " 15  IPO_Flag__Y_N_                    3650 non-null   object \n",
      " 16  Shares__Offered____in_this_Mkt    3650 non-null   float64\n",
      " 17  Secondary__Shs_Ofrd____in_this_M  970 non-null    float64\n",
      " 18  Yesterday_s_Stock_Price           3642 non-null   float64\n",
      " 19  Deal_Size__as_Pct_of_Market_Cap   3356 non-null   float64\n",
      " 20  Equity___Equity_Related_Simultan  3650 non-null   object \n",
      " 21  Common_Equity____mil_             2971 non-null   float64\n",
      " 22  Common_Equity_Before_the_Offerin  3472 non-null   float64\n",
      " 23  State                             3650 non-null   object \n",
      " 24  Overallot__Amt_Option___in_this_  3650 non-null   float64\n",
      " 25  Overallot__Amt_Sold___sum_of_all  1714 non-null   float64\n",
      " 26  Price_Current                     3640 non-null   float64\n",
      " 27  __Of_Insider_Shares_After_Offer   528 non-null    float64\n",
      " 28  __Of_Insider_Shares_Before_Offer  620 non-null    float64\n",
      " 29  Sec_Shrs_Filed_as___of_Shares     948 non-null    float64\n",
      " 30  Shareholder_Take_Up_All_Markets_  111 non-null    float64\n",
      " 31  Shares_Ofrd_as___of_Shs_Out_Bef_  2151 non-null   float64\n",
      " 32  Stock_Price_at_Close_of_Offer__0  3535 non-null   float64\n",
      "dtypes: float64(17), object(16)\n",
      "memory usage: 941.1+ KB\n"
     ]
    }
   ],
   "source": [
    "clean_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA for Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Select only numeric columns\n",
    "numeric_cols = clean_data.select_dtypes(include=['number'])\n",
    "\n",
    "# Step 2: Log-transform the numeric columns\n",
    "clean_data_log_transformed = numeric_cols.apply(np.log1p)\n",
    "\n",
    "# Step 3: Check if any NaN values were introduced after log transformation\n",
    "if clean_data_log_transformed.isna().sum().any():\n",
    "    print(\"NaN values found in the log-transformed data:\")\n",
    "    print(clean_data_log_transformed.isna().sum())\n",
    "    \n",
    "    # You can choose to drop rows or fill NaNs\n",
    "    clean_data_log_transformed = clean_data_log_transformed.dropna()  # or .fillna(0), depending on your choice\n",
    "\n",
    "# Step 4: Standardize the log-transformed data\n",
    "scaler = StandardScaler()\n",
    "numeric_scaled = scaler.fit_transform(clean_data_log_transformed)\n",
    "\n",
    "# Step 5: Apply PCA\n",
    "pca = PCA()\n",
    "pca_components = pca.fit_transform(numeric_scaled)\n",
    "\n",
    "# Step 6: Explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Step 7: Plot explained variance\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o')\n",
    "plt.title('Explained Variance by Each Principal Component')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.show()\n",
    "\n",
    "# Step 8: Create a DataFrame for the principal components (optional)\n",
    "pca_df = pd.DataFrame(pca_components, columns=[f'PC{i+1}' for i in range(pca_components.shape[1])])\n",
    "\n",
    "# Step 9: Display results\n",
    "print(\"Explained Variance by Component: \", explained_variance)\n",
    "print(\"Principal Components Data: \")\n",
    "print(pca_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Extract the top 3 principal components' loadings (variables' contributions)\n",
    "# The components_ attribute contains the loadings of the original features for each principal component\n",
    "loadings = pca.components_\n",
    "\n",
    "# Display the variables contributing to the first 3 principal components\n",
    "for i in range(3):\n",
    "    print(f\"\\nTop contributing variables to PC{i+1}:\")\n",
    "    # Create a DataFrame for easy viewing of loadings with feature names\n",
    "    loading_df = pd.DataFrame(loadings[i], index=numeric_cols.columns, columns=[f\"PC{i+1}\"])\n",
    "    # Sort the loadings by the absolute value, showing the highest contributing features\n",
    "    sorted_loading_df = loading_df.abs().sort_values(by=f\"PC{i+1}\", ascending=False)\n",
    "    print(sorted_loading_df.head(10))  # Display top 10 contributing features for each PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Extract absolute values of loadings\n",
    "loadings_df = pd.DataFrame(pca.components_, columns=numeric_cols.columns)\n",
    "\n",
    "# Step 2: Select top 3 principal components\n",
    "top_pcs = loadings_df.iloc[:3].T  # Transpose to get features as rows\n",
    "\n",
    "# Step 3: Compute absolute values for feature importance\n",
    "top_pcs_abs = top_pcs.abs()\n",
    "\n",
    "# Step 4: Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(top_pcs_abs, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Feature Importance in Top 3 Principal Components\")\n",
    "plt.xlabel(\"Principal Components\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract PCA loadings\n",
    "loadings_df = pd.DataFrame(pca.components_, columns=numeric_cols.columns)\n",
    "\n",
    "# Step 2: Select top 3 principal components and transpose\n",
    "top_pcs = loadings_df.iloc[:3].T  \n",
    "\n",
    "# Step 3: Plot with sign to see direction\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(top_pcs, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, center=0)\n",
    "plt.title(\"Feature Influence (Direction) in Top 3 Principal Components\")\n",
    "plt.xlabel(\"Principal Components\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking PCAs and Running Supervised Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply PCA and take the first 3 principal components\n",
    "pca_3_components = pca.transform(numeric_scaled)[:, :3]\n",
    "\n",
    "# Step 3: Create a DataFrame with the first 3 principal components\n",
    "pca_df = pd.DataFrame(pca_3_components, columns=['PC1', 'PC2', 'PC3'])\n",
    "\n",
    "# Step 4: Prepare target variable 'y'\n",
    "y = clean_data['Underpricing_Flag']  # Assuming this is the target variable\n",
    "\n",
    "# Step 5: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(pca_df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'LightGBM': LGBMClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Initialize results list\n",
    "results = []\n",
    "\n",
    "# Step 6: Train and evaluate the models\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    \n",
    "    # Print the metrics\n",
    "    print(f\"{model_name} - Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{model_name} - Recall: {recall:.4f}\")\n",
    "    print(f\"{model_name} - Precision: {precision:.4f}\")\n",
    "    print(f\"{model_name} - ROC AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Append results\n",
    "    results.append([model_name, accuracy, recall, precision, roc_auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Create comparison table\n",
    "results_df = pd.DataFrame(\n",
    "    results,\n",
    "    columns=['Model', 'Accuracy', 'Recall', 'Precision', 'ROC AUC']\n",
    ")\n",
    "print(\"\\nModel Performance Comparison Table:\")\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Create comparison table\n",
    "results_df = pd.DataFrame(\n",
    "    results,\n",
    "    columns=['Model', 'Accuracy', 'Recall', 'Precision', 'ROC AUC']\n",
    ")\n",
    "print(\"\\nModel Performance Comparison Table:\")\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting Threshold Value - Finding optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Function to calculate metrics at different thresholds\n",
    "def get_metrics_at_thresholds(model, X_test, y_test):\n",
    "    # Get predicted probabilities\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "    \n",
    "    # Calculate Youden's J statistic (tpr - fpr)\n",
    "    j_scores = tpr - fpr\n",
    "    \n",
    "    # Find the optimal threshold based on Youden's J statistic\n",
    "    optimal_threshold = thresholds[np.argmax(j_scores)]\n",
    "    \n",
    "    # Calculate the performance at the optimal threshold\n",
    "    y_pred_optimal = (y_pred_prob >= optimal_threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred_optimal)\n",
    "    recall = recall_score(y_test, y_pred_optimal)\n",
    "    precision = precision_score(y_test, y_pred_optimal)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    \n",
    "    return accuracy, recall, precision, roc_auc, optimal_threshold, j_scores, fpr, tpr, thresholds\n",
    "\n",
    "# Ensure X_train_resampled and y_train_resampled exist, or use X_train, y_train\n",
    "try:\n",
    "    X_train_resampled, y_train_resampled\n",
    "except NameError:\n",
    "    X_train_resampled, y_train_resampled = X_train, y_train  # Use original split if resampled is missing\n",
    "\n",
    "# Loop through each model and calculate metrics at the optimal threshold\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name} at optimal ROC threshold...\")\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Get metrics and the optimal threshold\n",
    "    accuracy, recall, precision, roc_auc, optimal_threshold, j_scores, fpr, tpr, thresholds = get_metrics_at_thresholds(model, X_test, y_test)\n",
    "    \n",
    "    # Print the metrics at the optimal threshold\n",
    "    print(f\"{model_name} - Optimal Threshold: {optimal_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Apply PCA and take the first 3 principal components\n",
    "pca_3_components = pca.transform(numeric_scaled)[:, :3]\n",
    "\n",
    "# Step 2: Create a DataFrame with the first 3 principal components\n",
    "pca_df = pd.DataFrame(pca_3_components, columns=['PC1', 'PC2', 'PC3'])\n",
    "\n",
    "# Step 3: Prepare target variable 'y'\n",
    "y = clean_data['Underpricing_Flag']  # Replace with your actual target if different\n",
    "\n",
    "# Step 4: Train-test split (80/20)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    pca_df, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'LightGBM': LGBMClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Function to get optimal threshold using Youden's J statistic\n",
    "def get_metrics_at_thresholds(model, X, y):\n",
    "    y_pred_prob = model.predict_proba(X)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y, y_pred_prob)\n",
    "    j_scores = tpr - fpr\n",
    "    optimal_idx = np.argmax(j_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold, y_pred_prob, fpr, tpr  # Return all for ROC plotting\n",
    "\n",
    "# Step 5: 5-fold CV with SMOTE to determine optimal thresholds\n",
    "skf = StratifiedKFold(n_splits=20)\n",
    "optimal_thresholds = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name} with 5-fold CV and SMOTE...\")\n",
    "    thresholds_per_fold = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X_train_full, y_train_full):\n",
    "        X_train = X_train_full.iloc[train_idx]\n",
    "        X_val = X_train_full.iloc[val_idx]\n",
    "        y_train = y_train_full.iloc[train_idx]\n",
    "        y_val = y_train_full.iloc[val_idx]\n",
    "        \n",
    "        # Apply SMOTE to training data within the fold\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "        \n",
    "        # Get optimal threshold\n",
    "        optimal_threshold, _, _, _ = get_metrics_at_thresholds(model, X_val, y_val)\n",
    "        thresholds_per_fold.append(optimal_threshold)\n",
    "    \n",
    "    # Store the average optimal threshold\n",
    "    optimal_thresholds[model_name] = np.mean(thresholds_per_fold)\n",
    "    print(f\"{model_name} - Average Optimal Threshold from CV: {optimal_thresholds[model_name]:.4f}\")\n",
    "\n",
    "# Step 6: Evaluate on test data using stored optimal thresholds\n",
    "results = []\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name} on Test Data with Optimal Threshold...\")\n",
    "    \n",
    "    # Retrain on full SMOTE-balanced training set\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_full, y_train_full)\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Use the optimal threshold\n",
    "    optimal_threshold = optimal_thresholds[model_name]\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred_optimal = (y_pred_prob >= optimal_threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred_optimal)\n",
    "    recall = recall_score(y_test, y_pred_optimal)\n",
    "    precision = precision_score(y_test, y_pred_optimal)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    \n",
    "    # Store results\n",
    "    results.append([model_name, accuracy, recall, precision, roc_auc])\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"{model_name} - Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "    print(f\"{model_name} - Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{model_name} - Recall: {recall:.4f}\")\n",
    "    print(f\"{model_name} - Precision: {precision:.4f}\")\n",
    "    print(f\"{model_name} - ROC AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_optimal)\n",
    "    print(f\"Confusion Matrix for {model_name}:\\n{cm}\")\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    _, _, fpr, tpr = get_metrics_at_thresholds(model, X_test, y_test)\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Finalize ROC plot\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Create comparison table\n",
    "results_df = pd.DataFrame(\n",
    "    results,\n",
    "    columns=['Model', 'Accuracy', 'Recall', 'Precision', 'ROC AUC']\n",
    ")\n",
    "print(\"\\nModel Performance Comparison Table:\")\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print confusion matrix for each model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nConfusion matrix for {model_name}:\")\n",
    "    print(confusion_matrix(y_test, model.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
